{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f375f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import prepare_data\n",
    "from tokenization.utils import build_asin_id_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77f8a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ee2b64cf154b46b2ad198358374bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01e52a0317b4793ab999a7a0657ad49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54624466c9a14c46976586360f4fac1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_valid, ds_test, asin2id = prepare_data(\"All_Beauty\")\n",
    "tokenizer = build_asin_id_tokenizer(asin2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86fc6ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '53 257 52 289 407 181 220 68 260 428 338 400 439 400 195 422 416 280 122 248 315 343 101 200 457 119 450',\n",
       " 'completion': '59'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d42e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2aeb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5444a9f92fa4d74933855d3768b74ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ced117680a4ac1afc07f73647521a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616be303570743dc99e2a8311c398322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guy hadad\\AppData\\Local\\Temp\\ipykernel_6908\\3745309389.py:242: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 481, 'bos_token_id': 480, 'pad_token_id': 479}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     2,395,424  (2.40M)\n",
      "Trainable parameters: 2,395,424  (2.40M)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='184' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [184/900 00:35 < 02:18, 5.17 it/s, Epoch 20.33/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy First Token</th>\n",
       "      <th>N Mistakes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.686253</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.697900</td>\n",
       "      <td>5.144432</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mistake 1] idx=0\n",
      "  pred_id=256, gold_id=129\n",
      "  pred_tok=['256']\n",
      "  gold_tok=['129']\n",
      "[mistake 2] idx=1\n",
      "  pred_id=458, gold_id=477\n",
      "  pred_tok=['458']\n",
      "  gold_tok=['477']\n",
      "[mistake 3] idx=2\n",
      "  pred_id=297, gold_id=38\n",
      "  pred_tok=['297']\n",
      "  gold_tok=['38']\n",
      "[mistake 1] idx=0\n",
      "  pred_id=256, gold_id=129\n",
      "  pred_tok=['256']\n",
      "  gold_tok=['129']\n",
      "[mistake 2] idx=1\n",
      "  pred_id=458, gold_id=477\n",
      "  pred_tok=['458']\n",
      "  gold_tok=['477']\n",
      "[mistake 3] idx=2\n",
      "  pred_id=297, gold_id=38\n",
      "  pred_tok=['297']\n",
      "  gold_tok=['38']\n"
     ]
    }
   ],
   "source": [
    "# tiny_qwen3_addition_for_prompts.py\n",
    "# Same training loop, now for datasets with {prompt, completion} and a prebuilt tokenizer.\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    Qwen3Config,\n",
    "    Qwen3ForCausalLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# -------------------------\n",
    "# 0) Repro\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); set_seed(SEED)\n",
    "\n",
    "# You already have:\n",
    "# ds_train, ds_valid, ds_test, asin2id = prepare_data(\"All_Beauty\")\n",
    "# tokenizer = build_asin_id_tokenizer(asin2id)\n",
    "\n",
    "# --- helper: robust encode for str OR list (ints/strs) ---\n",
    "def _encode_field(value, tok):\n",
    "    # If it's already a string: standard encode\n",
    "    if isinstance(value, str):\n",
    "        return tok(value, add_special_tokens=False)\n",
    "\n",
    "    # If it's a list (e.g., [123, 45, 6] or [\"B08..\", \"B07..\"]):\n",
    "    # 1) cast ints -> strings\n",
    "    # 2) tell HF these are pre-tokenized \"words\"\n",
    "    if isinstance(value, list):\n",
    "        tokens = [str(x) for x in value]\n",
    "        return tok(tokens, add_special_tokens=False, is_split_into_words=True)\n",
    "\n",
    "    # Anything else is unsupported\n",
    "    raise ValueError(f\"Unsupported field type for tokenization: {type(value)}\")\n",
    "\n",
    "# inverse map: id -> asin token\n",
    "id2asin = {v: k for k, v in asin2id.items()}\n",
    "\n",
    "# robust converter: value -> list[str] tokens (ASINs)\n",
    "def _to_token_words(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value]\n",
    "    if isinstance(value, int):\n",
    "        return [id2asin.get(value, str(value))]  # fallback: stringified id\n",
    "    if isinstance(value, list):\n",
    "        out = []\n",
    "        for x in value:\n",
    "            if isinstance(x, int):\n",
    "                out.append(id2asin.get(x, str(x)))\n",
    "            else:\n",
    "                out.append(str(x))\n",
    "        return out\n",
    "    raise ValueError(f\"Unsupported field type for tokenization: {type(value)}\")\n",
    "\n",
    "# ensure left padding + special tokens exist\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "if tokenizer.eos_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"<eos>\"})\n",
    "\n",
    "\n",
    "def tok_train(ex, tok):\n",
    "    enc = tok(_to_token_words(ex[\"prompt\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    dec = tok(_to_token_words(ex[\"completion\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    eos = tok.eos_token_id\n",
    "    input_ids = enc[\"input_ids\"] + dec[\"input_ids\"] + [eos]\n",
    "    labels    = [-100] * len(enc[\"input_ids\"]) + dec[\"input_ids\"] + [eos]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
    "\n",
    "def tok_eval(ex, tok):\n",
    "    enc = tok(_to_token_words(ex[\"prompt\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    dec = tok(_to_token_words(ex[\"completion\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    eos = tok.eos_token_id\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels    = dec[\"input_ids\"] + [eos]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) Collator (LEFT padding everywhere)\n",
    "# -------------------------\n",
    "def make_collate_fn(tokenizer):\n",
    "    LABEL_PAD_ID = -100\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    def collate_fn(features):\n",
    "        feats = [{\n",
    "            \"input_ids\": list(f[\"input_ids\"]),\n",
    "            \"attention_mask\": list(f[\"attention_mask\"]),\n",
    "            \"labels\": list(f[\"labels\"]),\n",
    "        } for f in features]\n",
    "\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in feats)\n",
    "\n",
    "        batch_input_ids, batch_attn, batch_labels = [], [], []\n",
    "        for f in feats:\n",
    "            L_in, L_lb = len(f[\"input_ids\"]), len(f[\"labels\"])\n",
    "            pad_in = max_len - L_in\n",
    "            pad_lb = max_len - L_lb\n",
    "\n",
    "            input_ids = [PAD_ID]*pad_in + f[\"input_ids\"]\n",
    "            attn      = [0]*pad_in     + f[\"attention_mask\"]\n",
    "            labels    = [LABEL_PAD_ID]*pad_lb + f[\"labels\"]\n",
    "\n",
    "            input_ids = input_ids[:max_len]\n",
    "            attn      = attn[:max_len]\n",
    "            labels    = labels[:max_len]\n",
    "\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attn.append(attn)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "# -------------------------\n",
    "# 3) Metrics: compare tail(pred) with labels length (no \"=\" assumption)\n",
    "# -------------------------\n",
    "def make_compute_metrics(tokenizer, max_print=3):\n",
    "    LABEL_PAD_ID = -100\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        n_total = 0\n",
    "        n_wrong = 0\n",
    "        printed = 0\n",
    "        skipped_empty = 0\n",
    "\n",
    "        for i, (p, l) in enumerate(zip(preds, labels)):\n",
    "            p = p.tolist() if hasattr(p, \"tolist\") else list(p)\n",
    "            l = l.tolist() if hasattr(l, \"tolist\") else list(l)\n",
    "\n",
    "            gold_ids = [t for t in l if t != LABEL_PAD_ID]\n",
    "            if not gold_ids:\n",
    "                skipped_empty += 1\n",
    "                continue\n",
    "\n",
    "            first_gold = gold_ids[0]\n",
    "            first_pred = p[len(p) - len(gold_ids)]  # first predicted token aligned to labels\n",
    "\n",
    "            n_total += 1\n",
    "            if first_pred != first_gold:\n",
    "                n_wrong += 1\n",
    "                if printed < max_print:\n",
    "                    print(f\"[mistake {printed+1}] idx={i}\\n\"\n",
    "                          f\"  pred_id={first_pred}, gold_id={first_gold}\\n\"\n",
    "                          f\"  pred_tok={tokenizer.convert_ids_to_tokens([first_pred])}\\n\"\n",
    "                          f\"  gold_tok={tokenizer.convert_ids_to_tokens([first_gold])}\")\n",
    "                    printed += 1\n",
    "\n",
    "        if n_total == 0:\n",
    "            print(f\"[metric] WARNING: no non-empty labels (skipped={skipped_empty}).\")\n",
    "            return {\"accuracy_first_token\": 0.0, \"n_mistakes\": 0}\n",
    "\n",
    "        acc = 1.0 - n_wrong / n_total\n",
    "        return {\"accuracy_first_token\": acc, \"n_mistakes\": n_wrong}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) Build tokenized datasets\n",
    "# -------------------------\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"validation\": ds_valid,\n",
    "    \"test\": ds_test,\n",
    "})\n",
    "\n",
    "tokenized_train = ds[\"train\"].map(lambda ex: tok_train(ex, tokenizer),\n",
    "                                  remove_columns=[\"prompt\", \"completion\"])\n",
    "tokenized_valid = ds[\"validation\"].map(lambda ex: tok_eval(ex, tokenizer),\n",
    "                                       remove_columns=[\"prompt\", \"completion\"])\n",
    "tokenized_test  = ds[\"test\"].map(lambda ex: tok_eval(ex, tokenizer),\n",
    "                                 remove_columns=[\"prompt\", \"completion\"])\n",
    "\n",
    "collate_fn = make_collate_fn(tokenizer)\n",
    "compute_metrics = make_compute_metrics(tokenizer)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Tiny model config\n",
    "# -------------------------\n",
    "scale = 16\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "count_params(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",                  # <- correct arg name\n",
    "    eval_steps=0.1,  # int steps\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    fp16=False, bf16=False,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d2c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EncodeRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
