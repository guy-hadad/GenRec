{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f375f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import prepare_data_like_other_filtering\n",
    "from tokenization.utils import build_asin_id_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77f8a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf7527e02db438c98da04851543ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d4048916ca4d259d9004f8f1e65a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dce710d19a46de96b216c7ddad7c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ds_train, ds_valid, ds_test, asin2id = prepare_data(\"All_Beauty\", K=0,  max_hist=50)\n",
    "ds_train, ds_valid, ds_test, asin2id = prepare_data_like_other_filtering(\n",
    "    name=\"All_Beauty\",         # or your domain\n",
    "    split_type=\"user\",         # or \"leave_one_out\"\n",
    "    ratios=(0.8, 0.1, 0.1),\n",
    "    max_hist=50,               # matches the second script's default\n",
    "    filter_by_meta=True,\n",
    ")\n",
    "tokenizer = build_asin_id_tokenizer(asin2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5276e5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_id', 'prompt', 'completion'],\n",
       "    num_rows: 38746\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2f58b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45280"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf7ca31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['prompt', 'completion']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m             items |= \u001b[38;5;28mset\u001b[39m(h.split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mItems BEFORE meta filter:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcount_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMeta items available:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(item2meta))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# After pruning histories (but BEFORE dropping rows), and after truncation:\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcount_items\u001b[39m\u001b[34m(ds_splits)\u001b[39m\n\u001b[32m      2\u001b[39m items = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     items |= \u001b[38;5;28mset\u001b[39m(\u001b[43mds_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mparent_asin\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m ds_splits[split][\u001b[33m'\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      6\u001b[39m         items |= \u001b[38;5;28mset\u001b[39m(h.split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\datasets\\arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\datasets\\arrow_dataset.py:2761\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2759\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2761\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2762\u001b[39m formatted_output = format_table(\n\u001b[32m   2763\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2764\u001b[39m )\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\datasets\\formatting\\formatting.py:604\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    602\u001b[39m         _raise_bad_key_type(key)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\datasets\\formatting\\formatting.py:541\u001b[39m, in \u001b[36m_check_valid_column_key\u001b[39m\u001b[34m(key, columns)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Column train not in the dataset. Current columns in the dataset: ['prompt', 'completion']\""
     ]
    }
   ],
   "source": [
    "def count_items(ds_splits):\n",
    "    items = set()\n",
    "    for split in ['train','valid','test']:\n",
    "        items |= set(ds_splits[split]['parent_asin'])\n",
    "        for h in ds_splits[split]['history']:\n",
    "            items |= set(h.split(' '))\n",
    "    return len(items)\n",
    "\n",
    "print(\"Items BEFORE meta filter:\", count_items(ds_train))\n",
    "print(\"Meta items available:\", len(item2meta))\n",
    "\n",
    "# After pruning histories (but BEFORE dropping rows), and after truncation:\n",
    "print(\"Items AFTER history-prune/truncate:\", count_items(truncated_datasets))\n",
    "print(\"#Items in mapping:\", len(data_maps['item2id']) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35d42e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf72083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f367b85b31b487ab9784597ff12bd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/505588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f6bef9c1f84a6d918fbf768f73f482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f35b48b5ef4e059fdd41b9219a986f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # combine prompt + completion\n",
    "    text = examples[\"prompt\"] + \" \" + examples[\"completion\"]\n",
    "    # truncate and return encoded inputs\n",
    "    return tokenizer(text, truncation=True, padding=False)\n",
    "\n",
    "tokenized_train = ds_train.map(tokenize_function)\n",
    "tokenized_valid = ds_valid.map(tokenize_function)\n",
    "tokenized_test = ds_test.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5263c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset, DatasetDict\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     DataCollatorWithPadding,\n",
    "#     Seq2SeqTrainer,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "# )\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # -------------------------\n",
    "# # 0) Tokenizer\n",
    "# # -------------------------\n",
    "# # use your tokenizer; must match the model's vocab\n",
    "# # tokenizer = ...\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# ds = DatasetDict(\n",
    "#     {\n",
    "#         \"train\": ds_train,\n",
    "#         \"validation\": ds_valid,\n",
    "#         \"test\": ds_test,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # -------------------------\n",
    "# # 2) Preprocess\n",
    "# #    - Train: loss only on completion (mask prompt with -100)\n",
    "# #    - Valid: evaluate last completion token (mask everywhere except last pos)\n",
    "# # -------------------------\n",
    "# def tok_ids(text):\n",
    "#     return tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# def preprocess_train(ex):\n",
    "#     p = tok_ids(ex[\"prompt\"])\n",
    "#     c = tok_ids(ex[\"completion\"]) + [tokenizer.eos_token_id]  # EOS so model learns to stop\n",
    "#     input_ids = p + c\n",
    "#     labels = ([-100] * len(p)) + c  # loss only on completion span\n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": [1] * len(input_ids),\n",
    "#         \"labels\": labels,\n",
    "#     }\n",
    "\n",
    "# def preprocess_valid_last_token(ex):\n",
    "#     p = tok_ids(ex[\"prompt\"])\n",
    "#     c = tok_ids(ex[\"completion\"])\n",
    "#     assert len(c) > 0, \"Completion must not be empty\"\n",
    "#     target_id = c[-1]\n",
    "\n",
    "#     # Context excludes the last token we want to predict\n",
    "#     context_ids = p + c[:-1]\n",
    "\n",
    "#     # Labels vector: only the LAST position holds target_id; others -100\n",
    "#     # (allows model to compute loss at that single position; also used to find the position)\n",
    "#     labels = ([-100] * (len(context_ids) - 1)) + [target_id]\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": context_ids,\n",
    "#         \"attention_mask\": [1] * len(context_ids),\n",
    "#         \"labels\": labels,\n",
    "#     }\n",
    "\n",
    "# tokenized_train = ds[\"train\"].map(preprocess_train, remove_columns=ds[\"train\"].column_names)\n",
    "# tokenized_valid = ds[\"validation\"].map(preprocess_valid_last_token, remove_columns=ds[\"validation\"].column_names)\n",
    "# tokenized_test  = ds[\"test\"].map(preprocess_valid_last_token, remove_columns=ds[\"test\"].column_names) if ds[\"test\"] else None\n",
    "\n",
    "# collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None)\n",
    "\n",
    "# # -------------------------\n",
    "# # 3) Metrics: Recall@100 on the last token\n",
    "# #    Use preprocess_logits_for_metrics to extract only the logits at the\n",
    "# #    single position where labels != -100, so compute_metrics sees [B, V].\n",
    "# # -------------------------\n",
    "# def preprocess_logits_for_metrics(logits, labels):\n",
    "#     # logits: [B, T, V] (Tensor); labels: [B, T] (Tensor) with exactly one non -100 per row\n",
    "#     if isinstance(logits, tuple):\n",
    "#         logits = logits[0]\n",
    "#     with torch.no_grad():\n",
    "#         # position of interest per row = wherever labels != -100\n",
    "#         # We guarantee exactly one position per row.\n",
    "#         mask = (labels != -100)  # [B, T]\n",
    "#         # Convert mask to indices\n",
    "#         idx = torch.argmax(mask.to(torch.int32), dim=1)  # [B]; OK because exactly one True\n",
    "#         batch_idx = torch.arange(logits.size(0), device=logits.device)\n",
    "#         # Slice logits at that position â†’ [B, V]\n",
    "#         selected = logits[batch_idx, idx]  # [B, V]\n",
    "#     return selected\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     # eval_pred.predictions: [B, V] float (numpy)\n",
    "#     # eval_pred.label_ids:   [B, T] ints where exactly one != -100 per row (the target id at last position)\n",
    "#     preds = eval_pred.predictions  # numpy [B, V]\n",
    "#     labels = eval_pred.label_ids   # numpy [B, T]\n",
    "#     # extract target ids\n",
    "#     # For each row, pick the element where label != -100\n",
    "#     target_ids = labels[labels != -100].reshape(-1)  # shape [B]\n",
    "\n",
    "#     # top-k\n",
    "#     k = 100\n",
    "#     topk_idx = np.argpartition(-preds, kth=min(k-1, preds.shape[1]-1), axis=1)[:, :k]  # [B, K] unordered within K\n",
    "#     # Turn into hits\n",
    "#     # (vectorized membership test)\n",
    "#     # Build a boolean matrix [B, K] whether target in topK\n",
    "#     hits = (topk_idx == target_ids[:, None]).any(axis=1).astype(np.float32)\n",
    "\n",
    "#     recall_at_100 = float(hits.mean()) if hits.size > 0 else 0.0\n",
    "#     return {\"recall@100_last_token\": recall_at_100}\n",
    "\n",
    "# # -------------------------\n",
    "# # 4) TrainingArguments (fix names)\n",
    "# # -------------------------\n",
    "# args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./qwen3_prompt_completion\",\n",
    "#     eval_strategy=\"steps\",      # <-- name is evaluation_strategy\n",
    "#     eval_steps=100,                   # integer number of steps\n",
    "#     logging_steps=50,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=5e-4,\n",
    "#     warmup_steps=0,\n",
    "#     weight_decay=0.0,\n",
    "#     report_to=\"none\",\n",
    "#     predict_with_generate=False,      # we want logits, not generated tokens\n",
    "#     include_inputs_for_metrics=False, # not needed\n",
    "#     fp16=False, bf16=False,\n",
    "#     seed=42,\n",
    "# )\n",
    "\n",
    "# # -------------------------\n",
    "# # 5) Trainer\n",
    "# # -------------------------\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=collate_fn,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_valid,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "# )\n",
    "\n",
    "# # Train + Eval\n",
    "# trainer.train()\n",
    "# print(\"Validation:\", trainer.evaluate())\n",
    "\n",
    "# # Optional: test with same metric\n",
    "# if tokenized_test is not None:\n",
    "#     print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c076d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# scale = 16\n",
    "# config = Qwen3Config(\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "#     hidden_size=max(1, int(4096/scale)),\n",
    "#     intermediate_size=max(1, int(22016/scale)),\n",
    "#     num_hidden_layers=max(1, int(32/scale)),\n",
    "#     num_attention_heads=max(1, int(32/scale)),\n",
    "#     num_key_value_heads=max(1, int(32/scale)),\n",
    "#     head_dim=max(1, int(128/scale)),\n",
    "# )\n",
    "# model = Qwen3ForCausalLM(config)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "# model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "# collate_fn = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,                 # optional but recommended\n",
    "#     label_pad_token_id=-100,     # ignore padded label positions\n",
    "#     pad_to_multiple_of=8,        # optional; good for Tensor Cores\n",
    "# )\n",
    "\n",
    "\n",
    "# args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./qwen3_prompt_completion\",\n",
    "#     eval_strategy=\"steps\",      # <-- name is evaluation_strategy\n",
    "#     eval_steps=100,                   # integer number of steps\n",
    "#     logging_steps=50,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=5e-4,\n",
    "#     warmup_steps=0,\n",
    "#     weight_decay=0.0,\n",
    "#     report_to=\"none\",\n",
    "#     predict_with_generate=False,      # we want logits, not generated tokens\n",
    "#     include_inputs_for_metrics=False, # not needed\n",
    "#     fp16=False, bf16=False,\n",
    "#     seed=42,\n",
    "# )\n",
    "\n",
    "# # -------------------------\n",
    "# # 5) Trainer\n",
    "# # -------------------------\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=collate_fn,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_valid,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "# )\n",
    "\n",
    "# # Train + Eval\n",
    "# trainer.train()\n",
    "# print(\"Validation:\", trainer.evaluate())\n",
    "\n",
    "# # Optional: test with same metric\n",
    "# if tokenized_test is not None:\n",
    "#     print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "441948e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# # DataLoader\n",
    "# collator = DataCollatorWithPadding(tokenizer)\n",
    "# valid_loader = DataLoader(ds_valid_proc, batch_size=32, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scale = 20\n",
    "# config = Qwen3Config(\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "#     hidden_size=max(1, int(4096/scale)),\n",
    "#     intermediate_size=max(1, int(22016/scale)),\n",
    "#     num_hidden_layers=max(1, int(32/scale)),\n",
    "#     num_attention_heads=max(1, int(32/scale)),\n",
    "#     num_key_value_heads=max(1, int(32/scale)),\n",
    "#     head_dim=max(1, int(128/scale)),\n",
    "# )\n",
    "# model = Qwen3ForCausalLM(config)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "# model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "# def count_params(m):\n",
    "#     total = sum(p.numel() for p in m.parameters())\n",
    "#     trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "#     print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "#     print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "# count_params(model)\n",
    "\n",
    "# # -------------------------\n",
    "# # 6) Train\n",
    "# # -------------------------\n",
    "# args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./qwen3_prompt_completion\",\n",
    "#     eval_strategy=\"steps\",                  # <- correct arg name\n",
    "#     eval_steps=0.1,  # int steps\n",
    "#     logging_steps=100,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     num_train_epochs=100,\n",
    "#     learning_rate=5e-4,\n",
    "#     warmup_steps=0,\n",
    "#     weight_decay=0.0,\n",
    "#     report_to=\"none\",\n",
    "#     predict_with_generate=True,\n",
    "#     include_inputs_for_metrics=False,\n",
    "#     fp16=False, bf16=False,\n",
    "#     seed=SEED,\n",
    "# )\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=collate_fn,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_valid,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# print(\"Validation:\", trainer.evaluate())\n",
    "# print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd2aeb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee886b4babba45768f6c89f9cf23de4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/505588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000898f6ab0e4e73b85bdf4b6facde23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c59d979d1ae46319d19b029b47ceb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guy hadad\\AppData\\Local\\Temp\\ipykernel_19920\\2835443081.py:242: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 112567, 'bos_token_id': 112566, 'pad_token_id': 112565}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     46,607,280  (46.61M)\n",
      "Trainable parameters: 46,607,280  (46.61M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='816' max='1580000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    816/1580000 04:13 < 136:45:47, 3.21 it/s, Epoch 0.05/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    222\u001b[39m args = Seq2SeqTrainingArguments(\n\u001b[32m    223\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./qwen3_prompt_completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,                  \u001b[38;5;66;03m# <- correct arg name\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     seed=SEED,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m    243\u001b[39m     model=model,\n\u001b[32m    244\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m    250\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate())\n\u001b[32m    254\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate(eval_dataset=tokenized_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tiny_qwen3_addition_for_prompts.py\n",
    "# Same training loop, now for datasets with {prompt, completion} and a prebuilt tokenizer.\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    Qwen3Config,\n",
    "    Qwen3ForCausalLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# -------------------------\n",
    "# 0) Repro\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); set_seed(SEED)\n",
    "\n",
    "# --- helper: robust encode for str OR list (ints/strs) ---\n",
    "def _encode_field(value, tok):\n",
    "    # If it's already a string: standard encode\n",
    "    if isinstance(value, str):\n",
    "        return tok(value, add_special_tokens=False)\n",
    "\n",
    "    # If it's a list (e.g., [123, 45, 6] or [\"B08..\", \"B07..\"]):\n",
    "    # 1) cast ints -> strings\n",
    "    # 2) tell HF these are pre-tokenized \"words\"\n",
    "    if isinstance(value, list):\n",
    "        tokens = [str(x) for x in value]\n",
    "        return tok(tokens, add_special_tokens=False, is_split_into_words=True)\n",
    "\n",
    "    # Anything else is unsupported\n",
    "    raise ValueError(f\"Unsupported field type for tokenization: {type(value)}\")\n",
    "\n",
    "# inverse map: id -> asin token\n",
    "id2asin = {v: k for k, v in asin2id.items()}\n",
    "\n",
    "# robust converter: value -> list[str] tokens (ASINs)\n",
    "def _to_token_words(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value]\n",
    "    if isinstance(value, int):\n",
    "        return [id2asin.get(value, str(value))]  # fallback: stringified id\n",
    "    if isinstance(value, list):\n",
    "        out = []\n",
    "        for x in value:\n",
    "            if isinstance(x, int):\n",
    "                out.append(id2asin.get(x, str(x)))\n",
    "            else:\n",
    "                out.append(str(x))\n",
    "        return out\n",
    "    raise ValueError(f\"Unsupported field type for tokenization: {type(value)}\")\n",
    "\n",
    "# ensure left padding + special tokens exist\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "if tokenizer.eos_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"<eos>\"})\n",
    "\n",
    "\n",
    "def tok_train(ex, tok):\n",
    "    enc = tok(_to_token_words(ex[\"prompt\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    dec = tok(_to_token_words(ex[\"completion\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    eos = tok.eos_token_id\n",
    "    input_ids = enc[\"input_ids\"] + dec[\"input_ids\"] + [eos]\n",
    "    labels    = [-100] * len(enc[\"input_ids\"]) + dec[\"input_ids\"] + [eos]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
    "\n",
    "def tok_eval(ex, tok):\n",
    "    enc = tok(_to_token_words(ex[\"prompt\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    dec = tok(_to_token_words(ex[\"completion\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    eos = tok.eos_token_id\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels    = dec[\"input_ids\"] + [eos]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) Collator (LEFT padding everywhere)\n",
    "# -------------------------\n",
    "def make_collate_fn(tokenizer):\n",
    "    LABEL_PAD_ID = -100\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    def collate_fn(features):\n",
    "        feats = [{\n",
    "            \"input_ids\": list(f[\"input_ids\"]),\n",
    "            \"attention_mask\": list(f[\"attention_mask\"]),\n",
    "            \"labels\": list(f[\"labels\"]),\n",
    "        } for f in features]\n",
    "\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in feats)\n",
    "\n",
    "        batch_input_ids, batch_attn, batch_labels = [], [], []\n",
    "        for f in feats:\n",
    "            L_in, L_lb = len(f[\"input_ids\"]), len(f[\"labels\"])\n",
    "            pad_in = max_len - L_in\n",
    "            pad_lb = max_len - L_lb\n",
    "\n",
    "            input_ids = [PAD_ID]*pad_in + f[\"input_ids\"]\n",
    "            attn      = [0]*pad_in     + f[\"attention_mask\"]\n",
    "            labels    = [LABEL_PAD_ID]*pad_lb + f[\"labels\"]\n",
    "\n",
    "            input_ids = input_ids[:max_len]\n",
    "            attn      = attn[:max_len]\n",
    "            labels    = labels[:max_len]\n",
    "\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attn.append(attn)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) Metrics: compare tail(pred) with labels length (no \"=\" assumption)\n",
    "# -------------------------\n",
    "def make_compute_metrics(tokenizer, max_print=3):\n",
    "    LABEL_PAD_ID = -100\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        n_total = 0\n",
    "        n_wrong = 0\n",
    "        printed = 0\n",
    "        skipped_empty = 0\n",
    "\n",
    "        for i, (p, l) in enumerate(zip(preds, labels)):\n",
    "            p = p.tolist() if hasattr(p, \"tolist\") else list(p)\n",
    "            l = l.tolist() if hasattr(l, \"tolist\") else list(l)\n",
    "\n",
    "            gold_ids = [t for t in l if t != LABEL_PAD_ID]\n",
    "            if not gold_ids:\n",
    "                skipped_empty += 1\n",
    "                continue\n",
    "\n",
    "            first_gold = gold_ids[0]\n",
    "            first_pred = p[len(p) - len(gold_ids)]  # first predicted token aligned to labels\n",
    "\n",
    "            n_total += 1\n",
    "            if first_pred != first_gold:\n",
    "                n_wrong += 1\n",
    "                if printed < max_print:\n",
    "                    print(f\"[mistake {printed+1}] idx={i}\\n\"\n",
    "                          f\"  pred_id={first_pred}, gold_id={first_gold}\\n\"\n",
    "                          f\"  pred_tok={tokenizer.convert_ids_to_tokens([first_pred])}\\n\"\n",
    "                          f\"  gold_tok={tokenizer.convert_ids_to_tokens([first_gold])}\")\n",
    "                    printed += 1\n",
    "\n",
    "        if n_total == 0:\n",
    "            print(f\"[metric] WARNING: no non-empty labels (skipped={skipped_empty}).\")\n",
    "            return {\"accuracy_first_token\": 0.0, \"n_mistakes\": 0}\n",
    "\n",
    "        acc = 1.0 - n_wrong / n_total\n",
    "        return {\"accuracy_first_token\": acc, \"n_mistakes\": n_wrong}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) Build tokenized datasets\n",
    "# -------------------------\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"validation\": ds_valid,\n",
    "    \"test\": ds_test,\n",
    "})\n",
    "\n",
    "tokenized_train = ds[\"train\"].map(lambda ex: tok_train(ex, tokenizer),\n",
    "                                  remove_columns=[\"prompt\", \"completion\"])\n",
    "tokenized_valid = ds[\"validation\"].map(lambda ex: tok_eval(ex, tokenizer),\n",
    "                                       remove_columns=[\"prompt\", \"completion\"])\n",
    "tokenized_test  = ds[\"test\"].map(lambda ex: tok_eval(ex, tokenizer),\n",
    "                                 remove_columns=[\"prompt\", \"completion\"])\n",
    "\n",
    "collate_fn = make_collate_fn(tokenizer)\n",
    "compute_metrics = make_compute_metrics(tokenizer)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Tiny model config\n",
    "# -------------------------\n",
    "scale = 20\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "count_params(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",                  # <- correct arg name\n",
    "    eval_steps=0.1,  # int steps\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    fp16=False, bf16=False,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "047d2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guy hadad\\AppData\\Local\\Temp\\ipykernel_6908\\1264666098.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 165912, 'bos_token_id': 165911, 'pad_token_id': 165910}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     42,740,872  (42.74M)\n",
      "Trainable parameters: 42,740,872  (42.74M)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1115300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      3/1115300 00:15 < 4723:07:50, 0.07 it/s, Epoch 0.00/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     26\u001b[39m args = Seq2SeqTrainingArguments(\n\u001b[32m     27\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./qwen3_prompt_completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,                  \u001b[38;5;66;03m# <- correct arg name\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     seed=SEED,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     47\u001b[39m     model=model,\n\u001b[32m     48\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     54\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate())\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate(eval_dataset=tokenized_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:498\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    496\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[32m    501\u001b[39m     loss=loss,\n\u001b[32m    502\u001b[39m     logits=logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m     attentions=outputs.attentions,\n\u001b[32m    506\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:67\u001b[39m, in \u001b[36mForCausalLMLoss\u001b[39m\u001b[34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[32m     66\u001b[39m shift_labels = shift_labels.to(logits.device)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m loss = \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:36\u001b[39m, in \u001b[36mfixed_cross_entropy\u001b[39m\u001b[34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfixed_cross_entropy\u001b[39m(\n\u001b[32m     29\u001b[39m     source: torch.Tensor,\n\u001b[32m     30\u001b[39m     target: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     **kwargs,\n\u001b[32m     34\u001b[39m ) -> torch.Tensor:\n\u001b[32m     35\u001b[39m     reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[39;00m\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(num_items_in_batch):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\nn\\functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scale = 32\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "count_params(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",                  # <- correct arg name\n",
    "    eval_steps=0.1,  # int steps\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    fp16=False, bf16=False,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f71d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea2353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EncodeRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
