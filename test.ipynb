{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f375f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import prepare_data\n",
    "from tokenization.utils import build_asin_id_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77f8a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c457dd2bb37482da716853e9080acfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba18ac438d604e0fbb7ac2e57ec67a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f094af3360ad46f99d696aa36113fac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_valid, ds_test, asin2id = prepare_data(\"Toys_and_Games\")\n",
    "tokenizer = build_asin_id_tokenizer(asin2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d42e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf72083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83de792f3f854190a4e15f5a73d365d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3224483e294e1ca430e9e153d1bc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4507967a2fa47d1acbb21c7fd043455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # combine prompt + completion\n",
    "    text = examples[\"prompt\"] + \" \" + examples[\"completion\"]\n",
    "    # truncate and return encoded inputs\n",
    "    return tokenizer(text, truncation=True, padding=False)\n",
    "\n",
    "tokenized_train = ds_train.map(tokenize_function)\n",
    "tokenized_valid = ds_valid.map(tokenize_function)\n",
    "tokenized_test = ds_test.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263c1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c91cdcb0fc45508308da985dcbc994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83eea370c2c54c0db05788dc67f835f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3003380a49d4263823e0d1c1281c016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mrecall@100_last_token\u001b[39m\u001b[33m\"\u001b[39m: recall_at_100}\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# 4) TrainingArguments (fix names)\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m args = \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./qwen3_prompt_completion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# <-- name is evaluation_strategy\u001b[39;49;00m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# integer number of steps\u001b[39;49;00m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# we want logits, not generated tokens\u001b[39;49;00m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_inputs_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# not needed\u001b[39;49;00m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# 5) Trainer\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    138\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m    139\u001b[39m     model=model,\n\u001b[32m    140\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n\u001b[32m    147\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# 0) Tokenizer\n",
    "# -------------------------\n",
    "# use your tokenizer; must match the model's vocab\n",
    "# tokenizer = ...\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "ds = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,\n",
    "        \"validation\": ds_valid,\n",
    "        \"test\": ds_test,\n",
    "    }\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Preprocess\n",
    "#    - Train: loss only on completion (mask prompt with -100)\n",
    "#    - Valid: evaluate last completion token (mask everywhere except last pos)\n",
    "# -------------------------\n",
    "def tok_ids(text):\n",
    "    return tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "def preprocess_train(ex):\n",
    "    p = tok_ids(ex[\"prompt\"])\n",
    "    c = tok_ids(ex[\"completion\"]) + [tokenizer.eos_token_id]  # EOS so model learns to stop\n",
    "    input_ids = p + c\n",
    "    labels = ([-100] * len(p)) + c  # loss only on completion span\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": [1] * len(input_ids),\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "def preprocess_valid_last_token(ex):\n",
    "    p = tok_ids(ex[\"prompt\"])\n",
    "    c = tok_ids(ex[\"completion\"])\n",
    "    assert len(c) > 0, \"Completion must not be empty\"\n",
    "    target_id = c[-1]\n",
    "\n",
    "    # Context excludes the last token we want to predict\n",
    "    context_ids = p + c[:-1]\n",
    "\n",
    "    # Labels vector: only the LAST position holds target_id; others -100\n",
    "    # (allows model to compute loss at that single position; also used to find the position)\n",
    "    labels = ([-100] * (len(context_ids) - 1)) + [target_id]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": context_ids,\n",
    "        \"attention_mask\": [1] * len(context_ids),\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_train = ds[\"train\"].map(preprocess_train, remove_columns=ds[\"train\"].column_names)\n",
    "tokenized_valid = ds[\"validation\"].map(preprocess_valid_last_token, remove_columns=ds[\"validation\"].column_names)\n",
    "tokenized_test  = ds[\"test\"].map(preprocess_valid_last_token, remove_columns=ds[\"test\"].column_names) if ds[\"test\"] else None\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Metrics: Recall@100 on the last token\n",
    "#    Use preprocess_logits_for_metrics to extract only the logits at the\n",
    "#    single position where labels != -100, so compute_metrics sees [B, V].\n",
    "# -------------------------\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    # logits: [B, T, V] (Tensor); labels: [B, T] (Tensor) with exactly one non -100 per row\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    with torch.no_grad():\n",
    "        # position of interest per row = wherever labels != -100\n",
    "        # We guarantee exactly one position per row.\n",
    "        mask = (labels != -100)  # [B, T]\n",
    "        # Convert mask to indices\n",
    "        idx = torch.argmax(mask.to(torch.int32), dim=1)  # [B]; OK because exactly one True\n",
    "        batch_idx = torch.arange(logits.size(0), device=logits.device)\n",
    "        # Slice logits at that position → [B, V]\n",
    "        selected = logits[batch_idx, idx]  # [B, V]\n",
    "    return selected\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred.predictions: [B, V] float (numpy)\n",
    "    # eval_pred.label_ids:   [B, T] ints where exactly one != -100 per row (the target id at last position)\n",
    "    preds = eval_pred.predictions  # numpy [B, V]\n",
    "    labels = eval_pred.label_ids   # numpy [B, T]\n",
    "    # extract target ids\n",
    "    # For each row, pick the element where label != -100\n",
    "    target_ids = labels[labels != -100].reshape(-1)  # shape [B]\n",
    "\n",
    "    # top-k\n",
    "    k = 100\n",
    "    topk_idx = np.argpartition(-preds, kth=min(k-1, preds.shape[1]-1), axis=1)[:, :k]  # [B, K] unordered within K\n",
    "    # Turn into hits\n",
    "    # (vectorized membership test)\n",
    "    # Build a boolean matrix [B, K] whether target in topK\n",
    "    hits = (topk_idx == target_ids[:, None]).any(axis=1).astype(np.float32)\n",
    "\n",
    "    recall_at_100 = float(hits.mean()) if hits.size > 0 else 0.0\n",
    "    return {\"recall@100_last_token\": recall_at_100}\n",
    "\n",
    "# -------------------------\n",
    "# 4) TrainingArguments (fix names)\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",      # <-- name is evaluation_strategy\n",
    "    eval_steps=100,                   # integer number of steps\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=False,      # we want logits, not generated tokens\n",
    "    include_inputs_for_metrics=False, # not needed\n",
    "    fp16=False, bf16=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Trainer\n",
    "# -------------------------\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "# Train + Eval\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "\n",
    "# Optional: test with same metric\n",
    "if tokenized_test is not None:\n",
    "    print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076d047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guy hadad\\AppData\\Local\\Temp\\ipykernel_12748\\1382232349.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 165912, 'bos_token_id': 165911, 'pad_token_id': 165910}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='33459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/33459 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "scale = 16\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "collate_fn = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,                 # optional but recommended\n",
    "    label_pad_token_id=-100,     # ignore padded label positions\n",
    "    pad_to_multiple_of=8,        # optional; good for Tensor Cores\n",
    ")\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",      # <-- name is evaluation_strategy\n",
    "    eval_steps=100,                   # integer number of steps\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=False,      # we want logits, not generated tokens\n",
    "    include_inputs_for_metrics=False, # not needed\n",
    "    fp16=False, bf16=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Trainer\n",
    "# -------------------------\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "# Train + Eval\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "\n",
    "# Optional: test with same metric\n",
    "if tokenized_test is not None:\n",
    "    print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441948e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# DataLoader\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "valid_loader = DataLoader(ds_valid_proc, batch_size=32, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scale = 20\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "count_params(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",                  # <- correct arg name\n",
    "    eval_steps=0.1,  # int steps\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    fp16=False, bf16=False,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2aeb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a2ccf07b6641d6b4b914d680391363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f1b9483efe425ba3dd44154fcbc7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b383cda0b4e748299bf509517c922c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guy hadad\\AppData\\Local\\Temp\\ipykernel_6908\\2576593374.py:242: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 165912, 'bos_token_id': 165911, 'pad_token_id': 165910}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     68,372,040  (68.37M)\n",
      "Trainable parameters: 68,372,040  (68.37M)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1115300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      5/1115300 00:29 < 2996:44:16, 0.10 it/s, Epoch 0.00/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    222\u001b[39m args = Seq2SeqTrainingArguments(\n\u001b[32m    223\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./qwen3_prompt_completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,                  \u001b[38;5;66;03m# <- correct arg name\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     seed=SEED,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m    243\u001b[39m     model=model,\n\u001b[32m    244\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m    250\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate())\n\u001b[32m    254\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate(eval_dataset=tokenized_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tiny_qwen3_addition_for_prompts.py\n",
    "# Same training loop, now for datasets with {prompt, completion} and a prebuilt tokenizer.\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    Qwen3Config,\n",
    "    Qwen3ForCausalLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# -------------------------\n",
    "# 0) Repro\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); set_seed(SEED)\n",
    "\n",
    "# --- helper: robust encode for str OR list (ints/strs) ---\n",
    "def _encode_field(value, tok):\n",
    "    # If it's already a string: standard encode\n",
    "    if isinstance(value, str):\n",
    "        return tok(value, add_special_tokens=False)\n",
    "\n",
    "    # If it's a list (e.g., [123, 45, 6] or [\"B08..\", \"B07..\"]):\n",
    "    # 1) cast ints -> strings\n",
    "    # 2) tell HF these are pre-tokenized \"words\"\n",
    "    if isinstance(value, list):\n",
    "        tokens = [str(x) for x in value]\n",
    "        return tok(tokens, add_special_tokens=False, is_split_into_words=True)\n",
    "\n",
    "    # Anything else is unsupported\n",
    "    raise ValueError(f\"Unsupported field type for tokenization: {type(value)}\")\n",
    "\n",
    "# inverse map: id -> asin token\n",
    "id2asin = {v: k for k, v in asin2id.items()}\n",
    "\n",
    "# robust converter: value -> list[str] tokens (ASINs)\n",
    "def _to_token_words(value):\n",
    "    if isinstance(value, str):\n",
    "        return [value]\n",
    "    if isinstance(value, int):\n",
    "        return [id2asin.get(value, str(value))]  # fallback: stringified id\n",
    "    if isinstance(value, list):\n",
    "        out = []\n",
    "        for x in value:\n",
    "            if isinstance(x, int):\n",
    "                out.append(id2asin.get(x, str(x)))\n",
    "            else:\n",
    "                out.append(str(x))\n",
    "        return out\n",
    "    raise ValueError(f\"Unsupported field type for tokenization: {type(value)}\")\n",
    "\n",
    "# ensure left padding + special tokens exist\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "if tokenizer.eos_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"<eos>\"})\n",
    "\n",
    "\n",
    "def tok_train(ex, tok):\n",
    "    enc = tok(_to_token_words(ex[\"prompt\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    dec = tok(_to_token_words(ex[\"completion\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    eos = tok.eos_token_id\n",
    "    input_ids = enc[\"input_ids\"] + dec[\"input_ids\"] + [eos]\n",
    "    labels    = [-100] * len(enc[\"input_ids\"]) + dec[\"input_ids\"] + [eos]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
    "\n",
    "def tok_eval(ex, tok):\n",
    "    enc = tok(_to_token_words(ex[\"prompt\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    dec = tok(_to_token_words(ex[\"completion\"]), add_special_tokens=False, is_split_into_words=True)\n",
    "    eos = tok.eos_token_id\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels    = dec[\"input_ids\"] + [eos]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) Collator (LEFT padding everywhere)\n",
    "# -------------------------\n",
    "def make_collate_fn(tokenizer):\n",
    "    LABEL_PAD_ID = -100\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    def collate_fn(features):\n",
    "        feats = [{\n",
    "            \"input_ids\": list(f[\"input_ids\"]),\n",
    "            \"attention_mask\": list(f[\"attention_mask\"]),\n",
    "            \"labels\": list(f[\"labels\"]),\n",
    "        } for f in features]\n",
    "\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in feats)\n",
    "\n",
    "        batch_input_ids, batch_attn, batch_labels = [], [], []\n",
    "        for f in feats:\n",
    "            L_in, L_lb = len(f[\"input_ids\"]), len(f[\"labels\"])\n",
    "            pad_in = max_len - L_in\n",
    "            pad_lb = max_len - L_lb\n",
    "\n",
    "            input_ids = [PAD_ID]*pad_in + f[\"input_ids\"]\n",
    "            attn      = [0]*pad_in     + f[\"attention_mask\"]\n",
    "            labels    = [LABEL_PAD_ID]*pad_lb + f[\"labels\"]\n",
    "\n",
    "            input_ids = input_ids[:max_len]\n",
    "            attn      = attn[:max_len]\n",
    "            labels    = labels[:max_len]\n",
    "\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_attn.append(attn)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) Metrics: compare tail(pred) with labels length (no \"=\" assumption)\n",
    "# -------------------------\n",
    "def make_compute_metrics(tokenizer, max_print=3):\n",
    "    LABEL_PAD_ID = -100\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        n_total = 0\n",
    "        n_wrong = 0\n",
    "        printed = 0\n",
    "        skipped_empty = 0\n",
    "\n",
    "        for i, (p, l) in enumerate(zip(preds, labels)):\n",
    "            p = p.tolist() if hasattr(p, \"tolist\") else list(p)\n",
    "            l = l.tolist() if hasattr(l, \"tolist\") else list(l)\n",
    "\n",
    "            gold_ids = [t for t in l if t != LABEL_PAD_ID]\n",
    "            if not gold_ids:\n",
    "                skipped_empty += 1\n",
    "                continue\n",
    "\n",
    "            first_gold = gold_ids[0]\n",
    "            first_pred = p[len(p) - len(gold_ids)]  # first predicted token aligned to labels\n",
    "\n",
    "            n_total += 1\n",
    "            if first_pred != first_gold:\n",
    "                n_wrong += 1\n",
    "                if printed < max_print:\n",
    "                    print(f\"[mistake {printed+1}] idx={i}\\n\"\n",
    "                          f\"  pred_id={first_pred}, gold_id={first_gold}\\n\"\n",
    "                          f\"  pred_tok={tokenizer.convert_ids_to_tokens([first_pred])}\\n\"\n",
    "                          f\"  gold_tok={tokenizer.convert_ids_to_tokens([first_gold])}\")\n",
    "                    printed += 1\n",
    "\n",
    "        if n_total == 0:\n",
    "            print(f\"[metric] WARNING: no non-empty labels (skipped={skipped_empty}).\")\n",
    "            return {\"accuracy_first_token\": 0.0, \"n_mistakes\": 0}\n",
    "\n",
    "        acc = 1.0 - n_wrong / n_total\n",
    "        return {\"accuracy_first_token\": acc, \"n_mistakes\": n_wrong}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) Build tokenized datasets\n",
    "# -------------------------\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"validation\": ds_valid,\n",
    "    \"test\": ds_test,\n",
    "})\n",
    "\n",
    "tokenized_train = ds[\"train\"].map(lambda ex: tok_train(ex, tokenizer),\n",
    "                                  remove_columns=[\"prompt\", \"completion\"])\n",
    "tokenized_valid = ds[\"validation\"].map(lambda ex: tok_eval(ex, tokenizer),\n",
    "                                       remove_columns=[\"prompt\", \"completion\"])\n",
    "tokenized_test  = ds[\"test\"].map(lambda ex: tok_eval(ex, tokenizer),\n",
    "                                 remove_columns=[\"prompt\", \"completion\"])\n",
    "\n",
    "collate_fn = make_collate_fn(tokenizer)\n",
    "compute_metrics = make_compute_metrics(tokenizer)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Tiny model config\n",
    "# -------------------------\n",
    "scale = 20\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "count_params(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",                  # <- correct arg name\n",
    "    eval_steps=0.1,  # int steps\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    fp16=False, bf16=False,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "047d2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guy hadad\\AppData\\Local\\Temp\\ipykernel_6908\\1264666098.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 165912, 'bos_token_id': 165911, 'pad_token_id': 165910}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     42,740,872  (42.74M)\n",
      "Trainable parameters: 42,740,872  (42.74M)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1115300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [      3/1115300 00:15 < 4723:07:50, 0.07 it/s, Epoch 0.00/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     26\u001b[39m args = Seq2SeqTrainingArguments(\n\u001b[32m     27\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./qwen3_prompt_completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,                  \u001b[38;5;66;03m# <- correct arg name\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     seed=SEED,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     47\u001b[39m     model=model,\n\u001b[32m     48\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     54\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate())\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest:\u001b[39m\u001b[33m\"\u001b[39m, trainer.evaluate(eval_dataset=tokenized_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:498\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    496\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[32m    501\u001b[39m     loss=loss,\n\u001b[32m    502\u001b[39m     logits=logits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m     attentions=outputs.attentions,\n\u001b[32m    506\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:67\u001b[39m, in \u001b[36mForCausalLMLoss\u001b[39m\u001b[34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[32m     66\u001b[39m shift_labels = shift_labels.to(logits.device)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m loss = \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:36\u001b[39m, in \u001b[36mfixed_cross_entropy\u001b[39m\u001b[34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfixed_cross_entropy\u001b[39m(\n\u001b[32m     29\u001b[39m     source: torch.Tensor,\n\u001b[32m     30\u001b[39m     target: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     **kwargs,\n\u001b[32m     34\u001b[39m ) -> torch.Tensor:\n\u001b[32m     35\u001b[39m     reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[39;00m\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(num_items_in_batch):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\guy hadad\\miniconda3\\envs\\EncodeRec\\Lib\\site-packages\\torch\\nn\\functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scale = 32\n",
    "config = Qwen3Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=max(1, int(4096/scale)),\n",
    "    intermediate_size=max(1, int(22016/scale)),\n",
    "    num_hidden_layers=max(1, int(32/scale)),\n",
    "    num_attention_heads=max(1, int(32/scale)),\n",
    "    num_key_value_heads=max(1, int(32/scale)),\n",
    "    head_dim=max(1, int(128/scale)),\n",
    ")\n",
    "model = Qwen3ForCausalLM(config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_new_tokens = 1  # adjust if your completions are longer\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total:,}  ({total/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable:,}  ({trainable/1e6:.2f}M)\")\n",
    "count_params(model)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train\n",
    "# -------------------------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./qwen3_prompt_completion\",\n",
    "    eval_strategy=\"steps\",                  # <- correct arg name\n",
    "    eval_steps=0.1,  # int steps\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,               # safer defaults; raise if you have GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=False,\n",
    "    fp16=False, bf16=False,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "print(\"Test:\", trainer.evaluate(eval_dataset=tokenized_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f71d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea2353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EncodeRec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
